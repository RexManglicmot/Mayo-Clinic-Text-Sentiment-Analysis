---
title: "Mayo Clinic Yelp Review Text Analysis"
author: RexManglicmot
output: 
  github_document: 
    toc: yes

always_allow_html: true
---

## Status: Contnuing Working document
Hi everyone. Iâ€™m continuing building my data analysis and R skills. As such, I would love feedback to better improve this project via rexmanglicmot@gmail.com. Any mistakes and misrepresentation of the data are my own.

Things Need to Do:

1. Head function on the text column shows only the 5th observation's text but not the others. Need to figure out why. I used the count function on text column and it shows 228 observations. Maybe this is an RStudio issue?
2. Need to create word clouds
3. Import and adjust dictionaries
4. Fix grammar
5. list of the metrics in US News for evaluating hospitals and pick up where they left off or, pick up on what is missing in their analyses. 

## Introduction
<center>

![](https://target.scene7.com/is/image/Target/GUEST_1ec18fe8-25da-4ae9-a754-33f87564fdcb?wid=725&hei=725&qlt=80&fmt=webp) 

</center>

What is the best hospital within the US? How do I know I'm getting the best care? These questions are sample of many as many Americans navigate the maze of the US private healthcare hospital system. With a total of 6,093 US hospitals^[https://www.aha.org/statistics/fast-facts-us-hospitals], it leaves many to wonder which healthcare system they should ut their lives and money in. Not all hospitals are created equal as some are privately and publiced owned; some do well versus some do not. 

Thus, every year U.S. News publishes the best hospitals ranked within the U.S.^[https://health.usnews.com/health-care/best-hospitals/articles/faq-how-and-why-we-rank-and-rate-hospitals] Although the list does not contain all the hospitals, but contained about 4500. The top ranked hospital for 2022-23 is the Mayo Clinic based on U.S. News methodology. To gain a better understanding why the Mayo Clinic is #1, I decided to use a Text Analysis on comments made by Yelp reviewers. 

Thus, this projects aims to understand user reviews through the Yelp platform, which is aimed to provide reviews to many businesses, including healthcare institutions (both private and public). By undergoing this research, there are 3 objectives:

1. Uncover hidden word-themes that reflect the true sentiments by patients that is also reproducible 
2. Provide a starting point for hospital C-suite to reflect some of the concerns patients have about their facility
3. Provide an opportunity for hospitals on the bottom part of the US News list to reflect on their services to patients 

By using text data, there are an array of methods to deploy. 

This project is comprised of the following chapters:

1. Webscraping Yelp Data
2. Loading the Libraries
3. Loading the data
4. Cleaning the Data
5. Exploratory Data Analysis
6. Word Cloud
7. Positive and Negative Words
8. Limitations
9. Conclusion
10. Inspiration for this project

## Webscraping Yelp Data
Yelp data was scrapped on via the [Yelp](https://www.yelp.com/) website. Within the search engine bar, I typed in "### *Mayo Clinic*" and used the first result to scrape the data. 

There were 228 reviews in total and the goal was to scrape all 228 reviews containing 4 metrics:

1. reviewer name
2. reviewer location
3. review rating
4. review text

After various hours debugging the code and looking at html tags to discern appropiate tags to scrape, I was able to obtain all reviews within Yelp. Troubleshooting code included unintentionally scraping a response from a Mayo Clinic official that accrued in more than the 228 reviews.

Nonetheless, below is the code used to scrape data. Mayo_Clinic.csv file is available within the repository. 
```{r Webscraping Yelp data, eval=FALSE}

#load libraries
library(tidyverse)
library(rvest)

#create an object to store the webpage address
url <- 'https://www.yelp.com/biz/mayo-clinic-rochester-12?osq=Mayo+clinic'

#convert the url to an html object for R processing
webpage <- read_html(url)

#create object to know page number on the webpage
webpageNum <- webpage %>%
  html_elements(xpath = "//div[@class= ' border-color--default__09f24__NPAKY text-align--center__09f24__fYBGO']") %>%
  html_text() %>%
  str_extract('of.*') %>%
  str_remove('of ') %>%
  as.numeric()

#create a sequence to iterate for page number
webpageSeq <- seq(from = 0, to = (webpageNum * 10)-10, by = 10)

#store items into empty objects
reviewer_name_all = c()
reviewer_location_all = c()
review_rating_all = c()
review_text_all = c()

#create a for loop to get values throughout the 23 pages
for (i in webpageSeq) {
  #need to create an if statement because the 1st page web address
  if (i == 0) {
    webpage <- read_html(url)
    #need to create else because webpage has more content than 1st page web 
    #address
  } else {
    webpage <- read_html(paste0(url, '&start=', i))
  }
  
  #reviewer name
  reviewer_name <- webpage %>%
    #return elements that I specify via xpath method
    #xpath is useful for locating elements
    #// means to search within entire document
    #* means to return any of the elements
    # div means to search within document with the div tags
    html_elements(xpath = "//div[starts-with(@class,' user-passport')]") %>%
    #look within a tag within the previous element
    html_elements(xpath = ".//a[starts-with(@href, '/user_details')]") %>%
    html_text()
  
  #reviewer location
  reviewer_location <- webpage %>%
    #location is within the same div tag, so use same code
    html_elements(xpath = "//div[starts-with(@class,' user-passport')]") %>%
    #location is also located within the span tag
    html_elements(xpath = ".//span[@class= ' css-qgunke']") %>%
    html_text() %>%
    #remove "Location" 
    #pipe remaining values that are not "Location"
    .[. !='Location']
  
  #review rating
  review_rating <- webpage %>%
    html_elements(xpath = "//div[starts-with(@class, ' review')]") %>%
    #within div tag there is an aria-label
    #contains function to look for aria-label that has rating  
    html_elements(xpath = "(.//div[contains(@aria-label, 'star rating')])[1]")%>%
    #ratings are not text, so must use different method and specify which 
    #attribute to obtain
    html_attr('aria-label') %>%
    #remove star rating
    str_remove_all(' star rating') %>%
    #convert into a numeric
    as.numeric()
  
  #review text
  review_text <- webpage %>%
    html_elements(xpath = "//div[starts-with(@class, ' review')]") %>%
    #look throughout webpage with the p tag
    #to get the first comment and not worr about business comment,
    #need to put in brackets
    html_elements(xpath = "(.//p[starts-with(@class, 'comment')])[1]") %>%
    #html_elements(xpath = ".//span[starts-with(@class, ' raw')]") %>%
    html_text()
  
  #appending to appropriate objects
  reviewer_name_all = append(reviewer_name_all, reviewer_name)
  reviewer_location_all = append(reviewer_location_all, reviewer_location)
  review_rating_all = append(review_rating_all, review_rating)
  review_text_all = append(review_text_all, review_text)
  
}

#create a dataframe containing appended values
Mayo_Clinic <- data.frame('name' = reviewer_name_all,
                          'location' = reviewer_location_all,
                          'rating' = review_rating_all,
                          'text'= review_text_all)

#view csv file
#head(Mayo_Clinic)
```

## Loading the Libraries

```{r, EDA, warning=FALSE, message=FALSE }
#load libraries
#install.packages('tidyverse') #had to re-install for some reason on 11/12/22
#install.packages('ggraph') #had to re-install for some reason on 11/12/22
#install.packages('DT')
#install.packages('gt')

library(tidyverse)
library(tidytext)
library(widyr)
library(RColorBrewer)
library(wordcloud)
library(igraph)
library(ggraph)
library(gt)
```

## Loading the Data
Now, let load the data. Instead of using the function str, let's expand my R code competency by using different functions
```{r, EDA 2}
#load Mayo Clinic data
data <- read.csv('Mayo_Clinic.csv')

#get the names of the variables
colnames(data)

#get the number of rows in the dataset
dim(data)
```
Ok, we see that there is the X variable which indexes the observations. Let's get rid of that column and check it if worked.

```{r}
#remove the X column
data <- data %>%
  select(-X)

#get the names other than the colnames funciton
names(data)
dim(data)
```


```{r, EDA 22}
#View first 10 observations of the data
#BUG why does it only show the 5th's obersvations text?
#it shows perfectly fine on RStudio
head(data, 15)

#how many observations are in the text column
length(data$text)
```


```{r}
#cbind(StrAlign(data, sep = "\\l"))
```



```{r, EDA 3}
#quick check to see if reviewer_name appears more than once
data %>%
  count(name, sort = TRUE)
```
5 people reviewed twice.

```{r EDA 4}
#quick check to see and sum if any NA are in rating variable
sum(is.na(data$rating))

#calculate summary statistics
summary(data$rating)
```
Looking at the summary statistics we see that the median and the max of the list is 5. What this means is that there are more of the 5 "ratings" in the list than there are others. The best way to view this, is via the barchart. 
```{r EDA 5}
#create a barchart to count the values
ggplot(data, aes(x=rating)) +
  geom_bar(color='black', fill='steelblue') +
  scale_fill_brewer(palette="Dark2")
```


```{r EDA 6}
#create dataframe to build stacked barchart
rating <- c(1,2,3,4,5)
total <- c((sum(data$rating == 1)),
           (sum(data$rating == 2)),
          (sum(data$rating == 3)),
          (sum(data$rating == 4)),
          (sum(data$rating == 5)))

#create a table of all the rating sums
table3 <-data.frame(rating, total)
print(table3)
```
The barchart is an tried and true plot used to plot discrete variables. Here, we see that the ratings from 2 to 4, in terms of count, are minimal comapred to 1 and 5;with the 5 rating being the most popular amongnst the reviewers. Next is to see the distribution of such. 

## WordCloud
```{r}

```


## Limitations

## Conclusions

## Appendix


